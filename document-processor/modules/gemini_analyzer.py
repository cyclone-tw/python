#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gemini æ™ºèƒ½åˆ†æå™¨ - å¢å¼·ç‰ˆ
å°ˆé–€ç‚ºå…¬æ–‡æ™ºèƒ½è™•ç†è¨­è¨ˆï¼Œæä¾›å®Œæ•´çš„åˆ†æåŠŸèƒ½
"""

import json
import re
import time
import sys
from typing import Dict, List, Optional
from datetime import datetime

# è¨­å®šç·¨ç¢¼ä»¥æ”¯æ´ä¸­æ–‡è¼¸å‡º
try:
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')
except:
    pass

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("âš ï¸ Gemini API å¥—ä»¶æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: pip install google-generativeai")

from config import SETTINGS, GEMINI_PROMPTS


class GeminiSmartAnalyzer:
    """Gemini æ™ºèƒ½åˆ†æå™¨"""

    def __init__(self, api_key: str = None):
        self.is_available = False
        self.model = None
        self.api_key = api_key if api_key is not None else SETTINGS['gemini']['api_key']

        if GEMINI_AVAILABLE and self.api_key:
            try:
                genai.configure(api_key=self.api_key)
                self.model = genai.GenerativeModel(SETTINGS['gemini']['model'])
                self.is_available = True
                print("âœ… Gemini API åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ Gemini API åˆå§‹åŒ–å¤±æ•—: {e}")

    def analyze_document(self, doc_info: Dict, progress_callback=None) -> Dict:
        """å…¨é¢åˆ†ææ–‡ä»¶"""
        if not self.is_available:
            return {'error': 'Gemini API æœªè¨­å®šæˆ–ç„¡æ³•ä½¿ç”¨'}

        try:
            if progress_callback:
                progress_callback("æ­£åœ¨ä½¿ç”¨ Gemini åˆ†ææ–‡ä»¶...")

            # æº–å‚™åˆ†æå…§å®¹
            content = self._prepare_content_for_analysis(doc_info)

            # å»ºç«‹æç¤ºè© - ä½¿ç”¨ f-string é¿å…æ ¼å¼åŒ–å•é¡Œï¼ˆåƒè€ƒåŸæœ¬æˆåŠŸçš„åšæ³•ï¼‰
            prompt = f"""è«‹å®Œæ•´åˆ†æä»¥ä¸‹å…¬æ–‡ï¼Œæä¾›çµæ§‹åŒ–çš„åˆ†æçµæœï¼š

{content}

è«‹æä¾›ä»¥ä¸‹åˆ†æï¼ˆä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼‰ï¼š

1. ç²¾ç°¡ä¸»æ—¨ï¼ˆ30å­—å…§ï¼Œå»é™¤å†—è©ï¼‰
2. æ–‡ä»¶é¡å‹ï¼ˆå¦‚ï¼šé€šçŸ¥ã€é‚€è«‹ã€èª¿æŸ¥ã€æœƒè­°ç­‰ï¼‰
3. é‡è¦æ€§ç­‰ç´šï¼ˆé«˜/ä¸­/ä½ï¼‰
4. é—œéµè¦é»ï¼ˆ3-5é»ï¼Œæ¯é»20å­—å…§ï¼‰
5. è¡Œå‹•é …ç›®ï¼ˆéœ€è¦åšä»€éº¼ï¼ŒåŒ…å«æˆªæ­¢æ—¥æœŸï¼‰
6. é‡è¦æ—¥æœŸï¼ˆæ´»å‹•æ—¥ã€æˆªæ­¢æ—¥ç­‰ï¼‰
7. è¯çµ¡è³‡è¨Šï¼ˆæ‰¿è¾¦äººã€é›»è©±ã€ä¿¡ç®±ï¼‰
8. å»ºè­°æª”åï¼ˆæ ¼å¼ï¼šYYYY-MM-DD_ç°¡æ½”ä¸»æ—¨ï¼‰
9. å»ºè­°æ­¸æª”è·¯å¾‘ï¼ˆåŸºæ–¼å…§å®¹æ€§è³ªå»ºè­°å­ç›®éŒ„ï¼‰
10. Googleæ•´åˆå»ºè­°ï¼ˆè¡Œäº‹æ›†æˆ–ä»»å‹™ï¼Ÿç‚ºä»€éº¼ï¼Ÿï¼‰

è«‹ä»¥JSONæ ¼å¼å›ç­”ï¼š
{{
  "refined_subject": "ç²¾ç°¡å¾Œçš„ä¸»æ—¨",
  "document_type": "æ–‡ä»¶é¡å‹",
  "priority": "é«˜/ä¸­/ä½",
  "key_points": ["è¦é»1", "è¦é»2", "è¦é»3"],
  "action_items": [
    {{"description": "è¡Œå‹•æè¿°", "deadline": "æˆªæ­¢æ—¥æœŸ", "priority": "é«˜/ä¸­/ä½"}}
  ],
  "important_dates": [
    {{"date": "æ—¥æœŸ", "description": "æè¿°", "type": "deadline/event"}}
  ],
  "contact_info": {{
    "name": "æ‰¿è¾¦äºº",
    "phone": "é›»è©±",
    "email": "ä¿¡ç®±"
  }},
  "suggested_filename": "å»ºè­°æª”åï¼ˆå«æ—¥æœŸå‰ç¶´ï¼‰",
  "suggested_path": "å»ºè­°æ­¸æª”å­ç›®éŒ„",
  "google_suggestion": {{
    "type": "calendar/task",
    "reason": "å»ºè­°åŸå› ",
    "title": "äº‹ä»¶æ¨™é¡Œ",
    "description": "è©³ç´°æè¿°",
    "due_date": "åˆ°æœŸæ—¥æœŸ"
  }}
}}"""

            # å‘¼å« Gemini API
            response = self._call_gemini_api(prompt)

            if 'error' in response:
                return response

            # è§£æä¸¦é©—è­‰å›æ‡‰
            analysis = self._parse_and_validate_response(response['text'], doc_info)

            if progress_callback:
                progress_callback("åˆ†æå®Œæˆï¼")

            return analysis

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            print(f"   âŒ åˆ†æéç¨‹è©³ç´°éŒ¯èª¤:")
            print(f"      {error_detail}")
            return {'error': f'åˆ†æéç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}'}

    def _prepare_content_for_analysis(self, doc_info: Dict) -> str:
        """æº–å‚™åˆ†æå…§å®¹"""
        content_parts = []

        # åŸºæœ¬è³‡è¨Š
        content_parts.append("ã€æ–‡ä»¶è³‡è¨Šã€‘")
        content_parts.append(f"æª”æ¡ˆåç¨±ï¼š{doc_info.get('file_name', '')}")

        # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤
        if doc_info.get('error'):
            content_parts.append(f"è™•ç†éŒ¯èª¤ï¼š{doc_info['error']}")

        # å…ƒæ•¸æ“š
        metadata = doc_info.get('metadata', {})
        if metadata:
            content_parts.append("\nã€å…¬æ–‡è³‡è¨Šã€‘")
            for key, value in metadata.items():
                if value and value.strip():
                    # é™åˆ¶å…ƒæ•¸æ“šé•·åº¦
                    if len(str(value)) > 200:
                        value = str(value)[:200] + "..."
                    content_parts.append(f"{key}ï¼š{value}")

        # ä¸»è¦å…§å®¹æ®µè½
        sections = doc_info.get('sections', {})
        exclude_sections = SETTINGS['pdf_processing']['exclude_sections']
        total_content_length = 0
        max_total_length = 8000  # ç¸½å…§å®¹é™åˆ¶

        for section_name, content in sections.items():
            if section_name not in exclude_sections and content and content.strip():
                content_parts.append(f"\nã€{section_name}ã€‘")

                # é™åˆ¶å–®å€‹æ®µè½é•·åº¦ï¼Œä¸¦è€ƒæ…®ç¸½é•·åº¦
                if len(content) > 1500:
                    content = content[:1500] + "..."

                content_parts.append(content)
                total_content_length += len(content)

                # å¦‚æœç¸½å…§å®¹å¤ªé•·ï¼Œåœæ­¢æ·»åŠ æ›´å¤šæ®µè½
                if total_content_length > max_total_length:
                    content_parts.append("\n[å…§å®¹éé•·ï¼Œå·²æˆªæ–·...]")
                    break

        # æ—¥æœŸè³‡è¨Š
        dates = doc_info.get('dates', [])
        if dates:
            content_parts.append("\nã€è­˜åˆ¥åˆ°çš„æ—¥æœŸã€‘")
            for date in dates[:5]:  # æœ€å¤šé¡¯ç¤º5å€‹æ—¥æœŸ
                if isinstance(date, dict):
                    content_parts.append(f"- {date.get('raw', date)} ({date.get('type', 'date')})")
                else:
                    content_parts.append(f"- {date}")

        # é™„ä»¶è³‡è¨Š
        attachments = doc_info.get('attachments', [])
        if attachments:
            content_parts.append("\nã€é™„ä»¶æª”æ¡ˆã€‘")
            for att in attachments[:10]:  # æœ€å¤šé¡¯ç¤º10å€‹é™„ä»¶
                content_parts.append(f"- {att['filename']}")
            if len(attachments) > 10:
                content_parts.append(f"- ... é‚„æœ‰ {len(attachments) - 10} å€‹é™„ä»¶")

        final_content = '\n'.join(content_parts)

        # æœ€çµ‚é•·åº¦æª¢æŸ¥
        if len(final_content) > 10000:
            final_content = final_content[:10000] + "\n\n[å…§å®¹å·²æˆªæ–·ä»¥ç¬¦åˆAPIé™åˆ¶]"

        print(f"   ğŸ“ æº–å‚™åˆ†æå…§å®¹ï¼Œé•·åº¦: {len(final_content)} å­—å…ƒ")
        return final_content

    def _call_gemini_api(self, prompt: str, max_retries: int = 3) -> Dict:
        """å‘¼å« Gemini APIï¼ŒåŒ…å«é‡è©¦æ©Ÿåˆ¶"""
        for attempt in range(max_retries):
            try:
                print(f"   ğŸ¤– å‘¼å« Gemini API (å˜—è©¦ {attempt + 1}/{max_retries})...")

                response = self.model.generate_content(
                    prompt,
                    generation_config=genai.GenerationConfig(
                        temperature=SETTINGS['gemini']['temperature'],
                        max_output_tokens=SETTINGS['gemini']['max_tokens'],
                    )
                )

                if response and hasattr(response, 'text') and response.text:
                    print(f"   âœ… API å›æ‡‰æˆåŠŸï¼Œé•·åº¦: {len(response.text)} å­—å…ƒ")
                    return {'text': response.text}
                elif response and hasattr(response, 'prompt_feedback'):
                    # æª¢æŸ¥æ˜¯å¦å› ç‚ºå®‰å…¨æ€§åŸå› è¢«é˜»æ“‹
                    feedback = response.prompt_feedback
                    if feedback.block_reason:
                        return {'error': f'å…§å®¹è¢«é˜»æ“‹: {feedback.block_reason}'}
                else:
                    print(f"   âš ï¸ API å›æ‡‰ç‚ºç©º")
                    return {'error': 'API å›æ‡‰ç‚ºç©º'}

            except Exception as e:
                error_msg = str(e)
                print(f"   âŒ API å‘¼å«å¤±æ•—: {error_msg}")

                # ç‰¹å®šéŒ¯èª¤è™•ç†
                if "quota" in error_msg.lower():
                    return {'error': f'API é…é¡ä¸è¶³: {error_msg}'}
                elif "api_key" in error_msg.lower():
                    return {'error': f'API Key éŒ¯èª¤: {error_msg}'}
                elif "permission" in error_msg.lower():
                    return {'error': f'æ¬Šé™éŒ¯èª¤: {error_msg}'}

                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt
                    print(f"   â³ {wait_time} ç§’å¾Œé‡è©¦...")
                    time.sleep(wait_time)
                else:
                    import traceback
                    print(f"   è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
                    return {'error': f'API å‘¼å«å¤±æ•—: {error_msg}'}

        return {'error': 'é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸'}

    def _parse_and_validate_response(self, response_text: str, doc_info: Dict) -> Dict:
        """è§£æä¸¦é©—è­‰ API å›æ‡‰"""
        try:
            print(f"   ğŸ” è§£æ API å›æ‡‰...")

            # æ¸…ç†å›æ‡‰æ–‡å­—
            original_text = response_text
            response_text = response_text.strip()

            # ç§»é™¤ markdown ä»£ç¢¼å¡Šæ¨™è¨˜
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.startswith('```'):
                response_text = response_text[3:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]

            # æ›´ç©æ¥µçš„æ¸…ç†
            response_text = response_text.strip()

            # å°‹æ‰¾JSONå…§å®¹
            # æœ‰æ™‚å€™å›æ‡‰æœƒæœ‰é¡å¤–çš„èªªæ˜æ–‡å­—ï¼Œæˆ‘å€‘éœ€è¦æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(0)

            print(f"   ğŸ“ æ¸…ç†å¾Œçš„JSONé•·åº¦: {len(response_text)} å­—å…ƒ")

            # è§£æ JSON
            analysis = json.loads(response_text)
            print(f"   âœ… JSON è§£ææˆåŠŸ")

            # é©—è­‰å’Œä¿®æ­£åˆ†æçµæœ
            analysis = self._validate_and_fix_analysis(analysis, doc_info)

            return analysis

        except json.JSONDecodeError as e:
            print(f"   âŒ JSON è§£æå¤±æ•—: {e}")
            print(f"   åŸå§‹å›æ‡‰å‰500å­—å…ƒ: {original_text[:500]}...")

            # å°‡å®Œæ•´å›æ‡‰å„²å­˜åˆ°æª”æ¡ˆä¾›æª¢æŸ¥
            try:
                with open('gemini_response_debug.txt', 'w', encoding='utf-8') as f:
                    f.write("=== åŸå§‹ Gemini å›æ‡‰ ===\n")
                    f.write(original_text)
                    f.write("\n\n=== æ¸…ç†å¾Œçš„å›æ‡‰ ===\n")
                    f.write(response_text)
                print(f"   ğŸ“ å®Œæ•´å›æ‡‰å·²å„²å­˜åˆ° gemini_response_debug.txt")
            except Exception as save_error:
                print(f"   âš ï¸ ç„¡æ³•å„²å­˜èª¿è©¦æª”æ¡ˆ: {save_error}")

            # å˜—è©¦ä¿®å¾©å¸¸è¦‹çš„JSONå•é¡Œ
            try:
                fixed_response = self._fix_json_response(response_text)
                if fixed_response:
                    analysis = json.loads(fixed_response)
                    print(f"   âœ… JSON ä¿®å¾©æˆåŠŸ")
                    return self._validate_and_fix_analysis(analysis, doc_info)
            except Exception as fix_error:
                print(f"   âš ï¸ JSON ä¿®å¾©ä¹Ÿå¤±æ•—: {fix_error}")

            # JSON è§£æå¤±æ•—ï¼Œä½¿ç”¨æ–‡å­—è§£æ
            print(f"   ğŸ”„ æ”¹ç”¨æ–‡å­—è§£æ...")
            return self._parse_text_response(original_text, doc_info)

        except Exception as e:
            print(f"   âŒ è§£æéç¨‹ç™¼ç”Ÿå…¶ä»–éŒ¯èª¤: {e}")
            return self._parse_text_response(original_text, doc_info)

    def _fix_json_response(self, response_text: str) -> Optional[str]:
        """å˜—è©¦ä¿®å¾©å¸¸è¦‹çš„JSONæ ¼å¼å•é¡Œ"""
        try:
            # ç§»é™¤å¯èƒ½çš„å‰å¾Œç©ºç™½å’Œç‰¹æ®Šå­—å…ƒ
            response_text = response_text.strip()

            # ä¿®å¾©å¸¸è¦‹çš„å¼•è™Ÿå•é¡Œ
            response_text = response_text.replace('"', '"').replace('"', '"')
            response_text = response_text.replace(''', "'").replace(''', "'")

            # ä¿®å¾©å¯èƒ½çš„æ›è¡Œå•é¡Œ
            response_text = re.sub(r'\n\s*"', '\n  "', response_text)

            # ç¢ºä¿JSONä»¥ { é–‹å§‹å’Œ } çµæŸ
            if not response_text.startswith('{'):
                start_idx = response_text.find('{')
                if start_idx != -1:
                    response_text = response_text[start_idx:]

            if not response_text.endswith('}'):
                end_idx = response_text.rfind('}')
                if end_idx != -1:
                    response_text = response_text[:end_idx + 1]

            return response_text

        except Exception:
            return None

    def _validate_and_fix_analysis(self, analysis: Dict, doc_info: Dict) -> Dict:
        """é©—è­‰å’Œä¿®æ­£åˆ†æçµæœ"""
        # ç¢ºä¿å¿…è¦æ¬„ä½å­˜åœ¨
        required_fields = [
            'refined_subject', 'document_type', 'priority', 'key_points',
            'action_items', 'important_dates', 'contact_info',
            'suggested_filename', 'suggested_path', 'google_suggestion'
        ]

        for field in required_fields:
            if field not in analysis:
                analysis[field] = self._get_default_value(field)

        # ä¿®æ­£æª”æ¡ˆåç¨±ï¼ˆç¢ºä¿æœ‰æ—¥æœŸå‰ç¶´ï¼‰
        if not self._has_date_prefix(analysis['suggested_filename']):
            date_str = self._get_date_string_from_doc(doc_info)
            subject = analysis.get('refined_subject', 'æœªçŸ¥æ–‡ä»¶')
            analysis['suggested_filename'] = f"{date_str}_{subject}"

        # æ¸…ç†æª”æ¡ˆåç¨±ä¸­çš„éæ³•å­—å…ƒ
        analysis['suggested_filename'] = self._clean_filename(analysis['suggested_filename'])

        # ç¢ºä¿å»ºè­°è·¯å¾‘ä¸ç‚ºç©º
        if not analysis.get('suggested_path'):
            analysis['suggested_path'] = 'ä¸€èˆ¬å…¬æ–‡'

        # é©—è­‰ Google å»ºè­°
        if not isinstance(analysis.get('google_suggestion'), dict):
            analysis['google_suggestion'] = {
                'type': 'task',
                'reason': 'ä¸€èˆ¬å…¬æ–‡å»ºè­°å»ºç«‹ä»»å‹™è¿½è¹¤',
                'title': analysis.get('refined_subject', 'å…¬æ–‡è™•ç†'),
                'description': 'è«‹æŸ¥çœ‹å…¬æ–‡å…§å®¹ä¸¦æ¡å–å¿…è¦è¡Œå‹•',
                'due_date': ''
            }

        return analysis

    def _get_default_value(self, field: str):
        """å–å¾—æ¬„ä½çš„é è¨­å€¼"""
        defaults = {
            'refined_subject': 'æœªçŸ¥ä¸»æ—¨',
            'document_type': 'ä¸€èˆ¬å…¬æ–‡',
            'priority': 'ä¸­',
            'key_points': [],
            'action_items': [],
            'important_dates': [],
            'contact_info': {},
            'suggested_filename': 'æœªå‘½åæ–‡ä»¶',
            'suggested_path': 'ä¸€èˆ¬å…¬æ–‡',
            'google_suggestion': {}
        }
        return defaults.get(field, '')

    def _has_date_prefix(self, filename: str) -> bool:
        """æª¢æŸ¥æª”åæ˜¯å¦æœ‰æ—¥æœŸå‰ç¶´"""
        return re.match(r'^\d{4}-\d{2}-\d{2}_', filename) is not None

    def _get_date_string_from_doc(self, doc_info: Dict) -> str:
        """å¾æ–‡ä»¶ä¸­å–å¾—æ—¥æœŸå­—ä¸²"""
        dates = doc_info.get('dates', [])
        if dates:
            date = dates[0]
            year = date['year'] + 1911
            month = str(date['month']).zfill(2)
            day = str(date['day']).zfill(2)
            return f"{year}-{month}-{day}"
        else:
            return datetime.now().strftime('%Y-%m-%d')

    def _clean_filename(self, filename: str) -> str:
        """æ¸…ç†æª”æ¡ˆåç¨±"""
        # ç§»é™¤éæ³•å­—å…ƒ
        illegal_chars = SETTINGS['file_processing']['illegal_chars']
        replacement_char = SETTINGS['file_processing']['replacement_char']

        cleaned = re.sub(illegal_chars, replacement_char, filename)

        # é™åˆ¶é•·åº¦
        max_length = SETTINGS['file_processing']['max_subject_length']
        if len(cleaned) > max_length:
            # ä¿ç•™æ—¥æœŸå‰ç¶´
            if self._has_date_prefix(cleaned):
                date_part = cleaned[:11]  # YYYY-MM-DD_
                subject_part = cleaned[11:max_length-11]
                cleaned = date_part + subject_part
            else:
                cleaned = cleaned[:max_length]

        return cleaned.strip(' ._-')

    def _parse_text_response(self, response_text: str, doc_info: Dict) -> Dict:
        """è§£æç´”æ–‡å­—å›æ‡‰ï¼ˆå‚™ç”¨æ–¹æ³•ï¼‰"""
        print("âš ï¸ JSON è§£æå¤±æ•—ï¼Œä½¿ç”¨æ–‡å­—è§£æ")

        analysis = {
            'refined_subject': 'æœªçŸ¥ä¸»æ—¨',
            'document_type': 'ä¸€èˆ¬å…¬æ–‡',
            'priority': 'ä¸­',
            'key_points': [],
            'action_items': [],
            'important_dates': [],
            'contact_info': {},
            'suggested_filename': self._get_date_string_from_doc(doc_info) + '_æœªå‘½åæ–‡ä»¶',
            'suggested_path': 'ä¸€èˆ¬å…¬æ–‡',
            'google_suggestion': {
                'type': 'task',
                'reason': 'æ–‡å­—è§£æå‚™ç”¨å»ºè­°',
                'title': 'å…¬æ–‡è™•ç†',
                'description': 'è«‹æª¢æŸ¥å…¬æ–‡å…§å®¹ä¸¦æ¡å–è¡Œå‹•',
                'due_date': ''
            }
        }

        # å˜—è©¦å¾æ–‡å­—ä¸­æå–ä¸€äº›åŸºæœ¬è³‡è¨Š
        lines = response_text.split('\n')
        for line in lines:
            line = line.strip()
            if 'ä¸»æ—¨' in line or 'æ¨™é¡Œ' in line:
                # å˜—è©¦æå–ä¸»æ—¨
                parts = line.split('ï¼š')
                if len(parts) > 1:
                    analysis['refined_subject'] = parts[1].strip()[:30]

        return analysis

    def generate_filename(self, subject: str, date: str = None, doc_type: str = None) -> str:
        """ç”Ÿæˆæª”æ¡ˆåç¨±"""
        if not self.is_available:
            return f"{date or datetime.now().strftime('%Y-%m-%d')}_{subject[:20]}"

        try:
            prompt = GEMINI_PROMPTS['filename_generation'].format(
                subject=subject,
                date=date or 'ä»Šå¤©',
                type=doc_type or 'ä¸€èˆ¬å…¬æ–‡'
            )

            response = self._call_gemini_api(prompt)
            if 'error' not in response:
                filename = response['text'].strip()
                return self._clean_filename(filename)

        except Exception as e:
            print(f"âš ï¸ æª”åç”Ÿæˆå¤±æ•—: {e}")

        # å‚™ç”¨æ–¹æ¡ˆ
        date_str = date or datetime.now().strftime('%Y-%m-%d')
        return f"{date_str}_{subject[:20]}"

    def suggest_directory(self, content: str) -> str:
        """å»ºè­°æ­¸æª”ç›®éŒ„"""
        if not self.is_available:
            return 'ä¸€èˆ¬å…¬æ–‡'

        try:
            prompt = GEMINI_PROMPTS['directory_suggestion'].format(content=content[:500])
            response = self._call_gemini_api(prompt)

            if 'error' not in response:
                directory = response['text'].strip()
                return directory if directory else 'ä¸€èˆ¬å…¬æ–‡'

        except Exception as e:
            print(f"âš ï¸ ç›®éŒ„å»ºè­°å¤±æ•—: {e}")

        return 'ä¸€èˆ¬å…¬æ–‡'


def test_analyzer():
    """æ¸¬è©¦åˆ†æå™¨åŠŸèƒ½"""
    # æª¢æŸ¥ API å¯ç”¨æ€§
    if not GEMINI_AVAILABLE:
        print("âŒ è«‹å®‰è£ google-generativeai å¥—ä»¶")
        return

    analyzer = GeminiSmartAnalyzer()
    if analyzer.is_available:
        print("âœ… GeminiSmartAnalyzer åˆå§‹åŒ–æˆåŠŸ")
    else:
        print("âŒ Gemini API è¨­å®šæœ‰å•é¡Œ")


if __name__ == "__main__":
    test_analyzer()